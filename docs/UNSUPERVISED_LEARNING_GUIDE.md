# Facebeak Unsupervised Learning Guide

## ğŸ§  Overview

This guide covers the new unsupervised learning capabilities that can dramatically improve Facebeak's crow identification performance while reducing manual labeling effort.

## ğŸš€ Features Implemented

### âœ… 1. Self-Supervised Pretraining (SimCLR)
- **What it does**: Pretrains your visual backbone on unlabeled crow crops using contrastive learning
- **Benefits**: More robust embeddings, better handling of lighting/pose variations
- **Use case**: Before training on manually labeled triplets

### âœ… 2. Clustering-Based Label Smoothing  
- **What it does**: Uses clustering to suggest crow merges and identify mislabeled samples
- **Benefits**: Semi-automatic labeling, catches human errors
- **Use case**: After each training run to improve data quality

### âœ… 3. Temporal Self-Consistency Loss
- **What it does**: Encourages similar embeddings for nearby frames in the same video
- **Benefits**: Smoother temporal tracking, more stable identities
- **Use case**: During training when you have frame number metadata

### âœ… 4. Auto-Labeling Low-Entropy Triplets
- **What it does**: Automatically suggests labels for high-confidence samples
- **Benefits**: Expands training data with minimal human effort
- **Use case**: Regularly after model updates to grow your dataset

### âœ… 5. Auto-Encoder Reconstruction Validation
- **What it does**: Detects outliers and mislabeled samples using reconstruction error
- **Benefits**: Quality control, identifies corrupted or multi-crow crops
- **Use case**: Data cleaning and validation

## ğŸ“¦ Installation & Setup

1. **Install additional dependencies** (if not already installed):
```bash
pip install scikit-learn matplotlib seaborn plotly
```

2. **Verify your database has temporal metadata**:
   - Frame numbers stored in embeddings
   - Video paths for temporal consistency

## ğŸ¯ Quick Start

### Option A: Use the Enhanced Training Script

```bash
# Basic usage with self-supervised pretraining
python train_with_unsupervised.py --epochs 100 --crop-dirs crow_crops/ videos/extracted_crops/

# Skip pretraining if you already have a good model
python train_with_unsupervised.py --epochs 50 --no-pretraining

# Use custom configuration
python train_with_unsupervised.py --config my_config.json --epochs 75
```

### Option B: Use the GUI Tools

```bash
# Launch the unsupervised learning GUI
python unsupervised_gui_tools.py
```

The GUI provides three main tabs:
- **Merge Suggestions**: Review and apply automatic crow merge recommendations
- **Outlier Review**: Identify and fix mislabeled or problematic samples  
- **Auto-Labeling**: Generate pseudo-labels for unlabeled data

## âš™ï¸ Configuration

Create `unsupervised_config.json`:

```json
{
  "embedding_dim": 512,
  "dropout_rate": 0.1,
  "margin": 1.0,
  "mining_type": "adaptive",
  "learning_rate": 0.001,
  "weight_decay": 0.01,
  "temporal_weight": 0.1,
  "max_frames_gap": 5,
  "simclr_epochs": 50,
  "simclr_batch_size": 32,
  "simclr_temperature": 0.07,
  "auto_label_confidence": 0.95,
  "distance_threshold": 0.3,
  "outlier_percentile": 95,
  "run_pretraining": true
}
```

### Key Parameters:

| Parameter | Description | Recommended Value |
|-----------|-------------|-------------------|
| `temporal_weight` | Weight for temporal consistency loss | 0.1 |
| `max_frames_gap` | Max frame difference for temporal pairing | 5 |
| `simclr_epochs` | Self-supervised pretraining epochs | 50 |
| `auto_label_confidence` | Threshold for auto-labeling | 0.95 |
| `outlier_percentile` | Percentile for outlier detection | 95 |

## ğŸ”„ Recommended Workflow

### Phase 1: Self-Supervised Pretraining
1. Collect all available crow crops (labeled and unlabeled)
2. Run SimCLR pretraining:
   ```bash
   python train_with_unsupervised.py --epochs 50 --crop-dirs crow_crops/
   ```
3. This creates a robust visual backbone before supervised training

### Phase 2: Enhanced Supervised Training  
1. Train with temporal consistency:
   ```bash
   python train_with_unsupervised.py --epochs 100 --no-pretraining
   ```
2. Monitor both triplet loss and temporal loss
3. Save checkpoints regularly

### Phase 3: Data Quality Improvement
1. Launch the GUI tools:
   ```bash
   python unsupervised_gui_tools.py
   ```
2. **Merge Suggestions Tab**:
   - Set confidence threshold (start with 0.8)
   - Click "Analyze Merges"
   - Review and apply high-confidence suggestions
   - Use "Review Images" for borderline cases

3. **Outlier Review Tab**:
   - Click "Find Outliers"
   - Review flagged samples
   - Mark as correct, relabel, or mark as "not crow"

4. **Auto-Labeling Tab**:
   - Set confidence threshold (0.95+ recommended)
   - Generate pseudo-labels
   - Review suggestions before adding to training data

### Phase 4: Iterative Improvement
1. Retrain with improved data
2. Repeat quality analysis
3. Monitor embedding quality metrics
4. Adjust thresholds based on results

## ğŸ“Š Monitoring & Metrics

### Training Metrics to Watch:
- **Triplet Loss**: Should decrease steadily
- **Temporal Loss**: Should be low and stable  
- **Clustering Quality Score**: Should improve over time
- **Pseudo-Labels Generated**: Indicates model confidence

### Quality Indicators:
```python
# Good signs:
- Silhouette score > 0.3
- Outlier rate < 10%
- High-confidence pseudo-labels available
- Temporal loss converging

# Warning signs:
- Many merge suggestions (may indicate poor initial labeling)
- High outlier rate (data quality issues)
- Temporal loss increasing (overfitting)
```

## ğŸ”§ Troubleshooting

### Common Issues:

**1. "Not enough unlabeled images for pretraining"**
- Solution: Collect more crop images or lower the threshold in the code
- Minimum: ~100 images for meaningful pretraining

**2. "No embeddings found for unsupervised analysis"**  
- Solution: Run detection + embedding extraction on some videos first
- Check database connectivity

**3. "Temporal loss is very high"**
- Solution: Reduce `temporal_weight` or increase `max_frames_gap`
- May indicate poor frame number metadata

**4. "No merge suggestions found"**
- Solution: Lower confidence threshold or check if you have multiple crows
- May indicate good data quality (not a problem!)

**5. "GUI crashes when displaying images"**
- Solution: Check image paths in database, ensure images exist
- Install PIL/Pillow correctly

## ğŸ›ï¸ Advanced Usage

### Custom Loss Functions:
```python
from unsupervised_learning import TemporalConsistencyLoss

# Create custom temporal loss
temporal_loss = TemporalConsistencyLoss(
    weight=0.2,  # Higher weight for more temporal smoothing
    max_frames_gap=10  # Consider larger gaps
)
```

### Manual Pseudo-Labeling:
```python
from unsupervised_learning import AutoLabelingSystem

auto_labeler = AutoLabelingSystem(confidence_threshold=0.9)
results = auto_labeler.generate_pseudo_labels(embeddings, labels)

# Review and selectively apply results
for idx, label in results['pseudo_labels'].items():
    confidence = results['confidences'][idx]
    if confidence > 0.95:
        # Apply this pseudo-label
        update_crow_label(idx, label)
```

### Batch Processing:
```python
# Process multiple videos for clustering analysis
from crow_clustering import CrowClusterAnalyzer

analyzer = CrowClusterAnalyzer()
for video_path in video_list:
    results = analyzer.cluster_crows(video_path)
    # Save results for later review
```

## ğŸ“ˆ Expected Improvements

With these unsupervised techniques, you should see:

1. **10-20% better embedding quality** (measured by silhouette score)
2. **30-50% reduction in manual labeling** (via auto-labeling)
3. **More stable temporal tracking** (via temporal consistency)
4. **Better generalization** to new lighting/pose conditions (via SimCLR)
5. **Cleaner training data** (via outlier detection)

## ğŸ”¬ Research Background

These techniques are based on:
- **SimCLR**: Chen et al., "A Simple Framework for Contrastive Learning"
- **Temporal Consistency**: Wang et al., "Temporal Consistency in Video"  
- **Clustering for SSL**: Caron et al., "Deep Clustering for Unsupervised Learning"
- **Outlier Detection**: Various AutoEncoder approaches

## ğŸ“ Citation

If you use these techniques in research, please cite Facebeak and the relevant papers above.

## ğŸ†˜ Getting Help

1. Check logs in `unsupervised_training.log`
2. Enable debug mode: `logging.basicConfig(level=logging.DEBUG)`
3. Create GitHub issues with error messages and configuration
4. Join the Facebeak community discussions

---

**Happy crow tracking! ğŸ¦â€â¬›** 